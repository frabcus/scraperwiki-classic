INSTALLING WITH BUILDOUT

[[Windows-specific installation notes in double square brackets throughout]]

Requirements
============

* Working python (2.5 or greater, not python 3)
* MySQL installed with root access (on Windows, v5.1)
* Mercurial
* (optional) python's virtualenv

Installation
============

The 'root' folder from now on will refer to the root of the mercurial checkout - in other words, the folder containing buildout, web, datastore etc. [In Windows, make sure there are no spaces in the folder names.]


1) Set up Virtualenv (optional, but reccomended)
************************************************

a) From within the root folder, run (note the dot at the end):

> $ virtualenv --no-site-packages .

This will create a new virtualenv with all its folders alongside the scraperwiki code. If virtualenv is not installed, you can download it from http://www.python.org/download or install using macports if you are using OSX. Note: the macports version needs to be called using "> virtualenv2.x  --no-site-packages ."

b) Now activate the virtualenv:

> $ source bin/activate

[[In Windows, type: > Scripts\activate.bat  ]]

Your shell will now show the virtualenv name "> (scraperwiki)$" at the prompt. This indicates the virtualenv is 'active'.  

YOU WILL TO DO THIS EACH TIME YOU OPEN A NEW SESSION IN YOUR SHELL. See http://pypi.python.org/pypi/virtualenv for more.

c) Then, get the older version of pip, version 0.6 breaks some old code. Go to [virtualenv]\Lib\site-packages and remove anything to with pip, then run 

> $ easy_install pip==0.5.1


2) Buildout

***********

To run the buildout, first bootstrap it:

> $ python bootstrap.py

Then run buildout [[except on Windows, see notes below]]:

> $ buildout 

[[ In Windows, this would be, : > "./bin/buildout.exe" - but in fact we need to skip it altogether, so just do "pip install xxx" for each 'install' command in the 'pip' section of buildout.cfg, EXCEPT python-mysql.
For python-mysql, download MySQL-python-1.2.2.win32-py2.6.exe from http://www.codegood.com/archives/4, install it on your machine, then copy all the python-mysql-related files from C:\Python26\Lib\site-packages or into the lib\site-packages directory of your virtualenv. ]]

This will download all the packages needed by scraperwiki.

If you get a weird error something like "setuptools.dist has no 'check_packages' attribute",
then run this command and then try buildout again:

> $ easy_install -U distribute


3) localsettings.py
*******************

Now edit make a copy of the local settings file

> $  cp localsettings.py-inhg  localsettings.py

Then edit localsettings.py to add details of your local setup (database etc). Call the database 'scraperwiki'. Edit HOME_DIR to your scraperwiki home directory. [[In Windows, do use forward slashes in the directory path.]]


4) run django
*************

Test the install by moving in to the web directory and running 'syncdb' and 'migrate':

> $ cd web
> $ python manage.py syncdb
> $ python manage.py migrate

Now load the fixtures:

> $ python manage.py loaddata ../fixtures/test_data.json

Now try running django:

> $ python manage.py runserver

Assuming no errors you should be able to visit http://localhost:8000/ and see the scraperwiki home page.


5) Initiate the UML
*******************

Instructions in UML/LOCAL explains that the system 
can run without fireboxing on a local machine for 
development.  

The FireBox subsystem can be run without UML by running 
the command

./runlocal


6) Create the scrapers repository
*******************

Create a directory called 'scrapers' and check out the scrapers into
that directory:

> hg clone scraperwiki@212.84.75.28/scrapers

Next, in MySQL, create a database called scraperwiki_datastore:
mysql> create database scraperwiki_datastore;

Then change to the datastore directory, and use mysql to import the database schema, scheme.sql:

(scraperwiki) C:\scraperwiki\web>cd c:\scraperwiki\scraperlibs\scraperwiki\datas
tore
(scraperwiki) C:\scraperwiki\scraperlibs\scraperwiki\datastore>mysql -u root -p -h localhost scraperwiki_datastore < scheme.sql

Finally, edit localsettings.py to tell it about the new database:

DATASTORE_DATABASE_ENGINE     = 'mysql'   
DATASTORE_DATABASE_HOST     = 'localhost'      
DATASTORE_DATABASE_NAME       = 'scraperwiki_datastore' 
DATASTORE_DATABASE_USER       = 'root'   
DATASTORE_DATABASE_PASSWORD   = 'yourpassword'  

And you should be done.
