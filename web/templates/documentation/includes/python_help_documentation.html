<div class="section">
  <p>In addition to all the standard Python libraries for downloading and parsing pages from the web, 
  ScraperWiki provides the following built in library.</p>
  
  <p>You can access it by including this at the top of your code:</p>

  <code>import scraperwiki</code>

  <p>The source code implementation of these functions can be found 
  <a href="https://bitbucket.org/ScraperWiki/scraperwiki/src/efd21ea1c875/scraperlibs/">here</a>.</p>
</div>

<h3><span id="sql"></span>The SQLite datastore</h3>
<p>ScraperWiki provides a fully-fledged <a href="http://www.sqlite.org/lang.html">SQLite</a> database for each scraper which you can save to.  
You can read the data back that has been committed by other scrapers, or extract it through 
the API</p>

<dl>
<dt>scraperwiki.<strong>sqlite.save</strong>(unique_keys, data[, table_name="swdata", verbose=2])</dt>
    <dd>Saves a data record into the datastore into the table given by table_name.  <br/>
    <dd>data is a dict object, unique_keys is a subset of data.keys() which determins 
        when a record is over-written.</dd>
    <dd>For large numbers of records data can be a list of dicts.
    </dd>

<dt>scraperwiki.<strong>sqlite.attach</strong>(name[, asname])</dt>
    <dd>Attaches to the datastore of another scraper with the given name.</dd>
    <dd>asname is an optional alias for the attached datastore.
    </dd>

<dt>scraperwiki.<strong>sqlite.select</strong>(val1[, val2], verbose=1)</dt>
    <dd>Executes a select command on the datastore, eg select("* from swdata limit 10")</dd>
    <dd>Returns a list of dicts that have been selected</dd>
    <dd>val2 is an optional list of parameters if the select command contains ?s
    </dd>

<dt>scraperwiki.<strong>sqlite.execute</strong>(val1[, val2, verbose=1])</dt>
    <dd>Executes any arbitrary sqlite command (except attach), eg create, delete, insert or drop</dd>
    <dd>val2 is an optional list of parameters if the select command contains question marks
    </dd>

<dt>scraperwiki.<strong>sqlite.commit</strong>()</dt>
    <dd>Commits to the file after a series of execute commands.  (sqlite.save auto-commits).
    </dd>

<dt>scraperwiki.<strong>sqlite.save_var</strong>(key, value)</dt>
    <dd>Saves an arbitrary single-value into a table called "swvariables". 
        Used to make scrapers able to continue after an interruption.
    </dd>

<dt>scraperwiki.<strong>sqlite.get_var</strong>(key[, default])</dt>
    <dd>Retrieves a single value that was saved by save_var.
    </dd>
</dl>




<h3>Geocoding functions</h3>
<p>Some installed functions to help you transform between different coordinate systems.</p> 

<dl>
<dt>scraperwiki.<strong>geo.extract_gb_postcode</strong>(string)</dt>
    <dd>Attempts to extract a UK postcode from a given string.
    </dd>

<dt>scraperwiki.<strong>geo.gb_postcode_to_latlng</strong>(postcode)</dt>
    <dd>Returns a WGS84 (lat, lng) pair for the central location of a UK postcode.
    </dd>

<dt>scraperwiki.<strong>geo.os_easting_northing_to_latlng</strong>(easting, northing)</dt>
    <dd>Converts a <a href="http://en.wikipedia.org/wiki/British_national_grid_reference_system">OSGB</a> 
        grid reference to a WGS84 (lat, lng) pair.
    </dd>

</dl>


<h3>Misc data format functions</h3>
<dl>

<dt>scraperwiki.<strong>pdftoxml</strong>(pdfdata)</dt>
    <dd>Convert a byte string containing a PDF file into an XML file containing the coordinates 
        and font of each text string.  <br/>
        Refer to 
        <a href="http://scraperwiki.com/scrapers/new/python?template=advanced-scraping-pdfs">the example</a> 
        for more details.
    </dd>

<dt>scraperwiki.<strong>utils.httpresponseheader</strong>("Content-Type", "image/PNG")</dt>
    <dd>Set the content-type header to something other than HTML when using a Scraperwiki "view" 
        (a simplified CGI script).
    </dd>

<dt>scraperwiki.<strong>utils.GET</strong>()</dt>
    <dd>The query_string map when using a Scraperwiki "view".
    </dd>

<dt>modulename = scraperwiki.<strong>utils.swimport</strong>(name)</dt>
    <dd>Imports the code from another scraper as the module "modulename".
    </dd>

<dt>scraperwiki.<strong>apiwrapper.getInfo</strong>(name)</dt>
    <dd>Get status and historical information about another scraper using an the API call</dd>
    <dd>(Used for generating email alerts about scraper history.)
    </dd>

</dl>

