<h2>The Datastore</h2>
<p>Every scraper comes with its own <a href="http://www.sqlite.org/lang.html">Sqlite database</a> 
which it can store data to.  You can also query data from other scrapers.</p>

<p>Easy save function for basic use.</p>
<code>ScraperWiki.save_sqlite(unique_keys=["a"], data={"a"=>1, "bbb"=>"Hi there"})
</code>

<p>If the values for the unique_keys matches a record already there, it will over-write</p>
<code>ScraperWiki.save_sqlite(unique_keys=["a"], data={"a"=>1, "bbb"=>"Bye there"})
</code>

<p>You can add new columns into the database and the table will extend automatically.
(The print is so you can see the comment.)</p>
<code>require 'pp'
pp ScraperWiki.save_sqlite(unique_keys=["a"], data={"a"=>2, "bbb"=>"Going home", "cccnew"=>-999.9})
</code>

<p>Each new column is given an <a href="http://www.sqlite.org/datatype3.html#affinity">affinity</a> 
according to the type of the value it is first given (text, integer, real).  It is okay to save a string 
in a column that was defined as an integer, but it will sometimes be converted if possible.
You can define a column with no affinity by giving it the name ending in "_blob".</p>
<code>ScraperWiki.save_sqlite(unique_keys=["a"], data={"a"=>1, "dddd_blob"=>"999.999"})
pp ScraperWiki.save_sqlite("swdata")
</code>

<p>Further parameters in the save function are table_name (the default table name is "swdata"), 
and verbose (which doesn't send messages to the data tab if set to 0</p>
<code>ScraperWiki.save_sqlite(unique_keys, data, table_name="swdata", verbose=2)
</code>

<p>You can also list a list of dicts in the save for greater speed</p>
<code>data = { {"a"=>10}, {"a"=>20}, {"a"=>30} ]
ScraperWiki.save_sqlite(["a"], data)
</code>

<p>To see the dict of table_names mapping to schemas.</p>
<code>pp ScraperWiki.show_tables()
</code>

<p>Info about a particular table (and its elements) can be made 
<code>info = ScraperWiki.table_info(name="swdata")
for column in info
    puts column.name +" "+ column.type
end
</code>

<p>You can execute direct SQL commands.  Back-ticks ` are used to quote column names that are have spaces in them.</p>
<code>ScraperWiki.sqliteexecute("create table ttt (xx int, `yy` string)")
ScraperWiki.sqliteexecute("insert into ttt values (?,?)", [9, 'hello'])
ScraperWiki.sqliteexecute("insert or replace into ttt values (:xx, :yy)", {"xx"=>10, "yy"=>"again"})
</code>

<p>Don't forget after doing your inserts you need to commit the result.  (The save() command always automatically commits.)</p>
<code>ScraperWiki.sqlitecommit()
</code>

<p>Selection can be done by execution of a select function.</p>
<code>pp ScraperWiki.sqliteexecute("select * from ttt")
pp ScraperWiki.sqliteexecute("select min(xx), max(xx) from ttt group by yy")
</code>

<p>The result will be a dict with a list for keys, and a list of rows (which are lists) for the 
corresponding values.</p>
<code>{ "keys"=> ["xx", "yy"], data=>[[9, 'hello'], [10, 'again']] }
</code>

<p>The shorthand select command gives the results in dicts.</p>
<code>print scraperwiki.sqlite.select("* from ttt")
[{'yy'=> 'hello', 'xx'=> 9}, {'yy'=>'again', 'xx'=>10}]
</code>

<p>You can also clean up by deleting rows or dropping tables</p>
<code>ScraperWiki.sqliteexecute("delete from ttt where xx=9")
ScraperWiki.sqliteexecute("drop table if exists ttt")
</code>

<p>To access data from other scrapers we attach to them.</p>
<code>ScraperWiki.attach("new_americ_foundation_drone_strikes")
pp ScraperWiki.select("* from new_americ_foundation_drone_strikes.swdata limit 2")
</code>

<p>To make it easy, you can change the name of the database you import it as.</p>
<code>ScraperWiki.attach("new_americ_foundation_drone_strikes", "src")
pp ScraperWiki.table_info("src.swdata")
</code>

<p>Access to other scrapers data through the attach interface is read-only.</p>
