<div class="content col left">
<h3>Scraperwiki Python documentation</h3>

<p>In your scrapers, you can use ScraperWiki's in-built functions for retrieving, geocoding and saving data. 
Just add <code>import scraperwiki</code> at the top of your scraper.</p>


<div class="section">
<h3><span id="scraping"></span>scraperwiki.scrape</h3>
<p>The most basic function. Downloads a web page, and returns you the HTML as a string.
<strong>Note:</strong> This is just a convenience function. If you want to do
anything more complex, you can use other Python download modules directly, such as 
<a href="http://docs.python.org/library/urllib2.html">urllib2</a>.
</p>
    <h4>Method</h4>
    <table class="api_detail example_table">
    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
<tr><td><code>scraperwiki.scrape<br/>&nbsp;(url, [params=], )</code></td><td>String</td>
    <td><dl>
    <dt><code>url</code></dt>
        <dd>String. The address of the web page, e.g. <code>http://www.google.co.uk</code>
        </dd>
    <dt><code>params</code> (optional)</dt>
<dd>Dictionary. If present, sends a POST request, with the parameters specified in the dictionary. 
        </dd>
    </dl></td>
    </tr>
    </table>
	<h4>Example</h4>
	<div class="example"><code>
	import scraperwiki<br/>
	html = scraperwiki.scrape('http://www.google.co.uk') # GET request.<br/>
	html = scraperwiki.scrape('http://www.google.co.uk', { 'key' : value })	# POST request.
	</code></div>



	        <div class="section">
	            <h3><span id="sql"></span>The SQLite datastore</h3>
	            <p>ScraperWiki provides a fully-fledged SQLite database to save your data into.
                You do not need to define the table schemas if you use the <code>scraperwiki.sqlite.save</code> function.</p>
	            <p>However to retrieve the data you may need to use some basic SQL.  
                It is possible to attach to the database of another scraper and query its data too.  
                This can be useful for looking up shared object identifiers.</p>
				    <h4>Method</h4>
				    <table class="api_detail example_table">
				    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
					<tr><td><code>scraperwiki.sqlite.save<br/>
					    &nbsp;(unique_keys, data,<br/>
					    &nbsp;[table_name="swdata"],<br/>
					    &nbsp;[verbose=2]) </code></td>
					<td>None</td>
				    <td>
				    <p>Stores a new record in the SQLite database, or replaces an existing
	                record which has the same unique keys. The data parameter is a
	                dictionary containing all the attributes of your record and their
	                values.
	                </p>
	                <dl>
	                    <dt><code>unique_keys</code></dt>
	                        <dd>List. Specify the attributes in
	                        your record which are together unique, e.g. <code>['dog_id']</code> or
	                        <code>['cattery', 'cat_number']</code>. Any existing data with
	                        the same values for those attributes as your new data
	                        will be overwritten. Must contain at least one element.
                            This list defines an index to your table
	                        </dd>
	                    <dt><code>data</code></dt>
	                        <dd>Dictionary or list of dictionaries. The data defining the record (or list of records).
                            Values other than integers and floats are converted to strings.
                            The data table is altered to include all the keys you give it, and values you do not set 
                            are set to Null.
	                        </dd>
	                    <dt><code>table_name</code> (optional)</dt>
	                        <dd>The default table name is "swdata", but you can set different names and create 
                            multiple tables in the same database.  
	                        </dd>
	                    <dt><code>verbose</code> (optional)</dt>
	                        <dd>Set this to 0 to prevent the data replaying into the data tab when 
                            you are running it in the editor.
	                        </dd>
	                </dl>
				    </td>
				    </tr>
					<tr><td><code>scraperwiki.sqlite.execute<br/>
					    &nbsp;(val1, [val2=])</code></td>
					<td>Dict</td>
				    <td>
				    <p>Executes an SQLite command.
	                </p>
	                <dl>
	                    <dt><code>val1</code></dt>
	                        <dd>String. The SQLite command itself. See the 
                          <a href="http://www.sqlite.org/docs.html">SQLite documentation</a> for 
                          command syntax.
                          Returns an object <code>{ keys:[], data:[ [ ], [ ], ... ] }</code>
	                        </dd>
	                    <dt><code>val2</code> (optional)</dt>
	                        <dd>List. Used if your sql command is parametrized, eg
                            <code>val1="insert into swdata values (?,?,?)"</code>, 
                            then possible to have <code>val2=["a", 1, "thing"]</code>.
	                        </dd>
	                </dl>
				    </td>
				    </tr>
					<tr><td><code>scraperwiki.sqlite.commit<br/>
					    &nbsp;()</code></td>
					<td>None</td>
				    <td>
				    <p>Commits your SQLite changes. Required if you are using 
                    <code>scraperwiki.sqlite.execute</code> to enter data rather than 
                    <code>scraperwiki.sqlite.save</code>, which autocommits for you.
	                </p>
				    </td>
				    </tr>				
					<tr><td><code>scraperwiki.sqlite.attach<br/>
					    &nbsp;(name, [asname=])</code></td>
					<td>None</td>
				    <td>
				    <p>Connects to the sqlite database of another scraper
	                </p>
	                <dl>
	                    <dt><code>name</code></dt>
	                        <dd>String. The name of the other scraper
	                        </dd>
	                    <dt><code>asname</code> (optional)</dt>
	                        <dd>String. Assigns a different name to the scraper you have attached to.
                            eg <code>attach("welsh_assembly_expenses", "src")</code> means we can 
                            access the tables in it as <code>src.swdata</code>.
	                        </dd>
	                </dl>
				    </td>
				    </tr>
				    </table>
					<h4>Example</h4>
					<div class="example"><code>
					scraperwiki.sqlite.save(['name'], data)<br/>
					person_fields = ["booking_id INTEGER PRIMARY KEY","name TEXT"]<br/>
					scraperwiki.sqlite.execute("create table if not exists person (%s)" \<br/>
					&nbsp;&nbsp;% ",".join(person_fields))<br/>
					scraperwiki.sqlite.execute("INSERT into 'person' values(?,?)", (100, 'Dave'))<br/>
					scraperwiki.sqlite.commit()<br/>
					results = scraperwiki.sqlite.select("booking_id from person where name='Dave'")
					</code></div>
	            </div>



	    <h2><span id="variables"></span>Managing scrapers with persistent variables</h2> 
            <p>
              Scrapers can store and retrieve single variables into the database to help 
              them pick up where they left off if interrupted on a very large scrape.
            </p>
			    <h4>Method</h4>
			    <table class="api_detail example_table">
			    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
			<tr><td><code>scraperwiki.sqlite.save_var<br/>&nbsp;(key, value)</code></td><td>String</td>
			    <td>
			 <p>Stores an item of metadata that can later be retrieved using the <code>scraperwiki.sqlite.get_var</code> function.
                Values are put into a table called "swvariables".</p>
			    <dl>
	              <dt><code>key</code></dt>
	              <dd>
	                This is a unique identifier for the data being stored. Multiple save requests
	                with the same key will overwrite any existing value associated with the key.
                    
	              </dd>
	              <dt><code>value</code></dt>
	              <dd>The value to be stored.</dd>
	            </dl></td></tr>
	            <tr>
				<td><code>scraperwiki.sqlite.get_var<br/>&nbsp;(key, [default=])</code></td><td>String</td>
				 <td>
				 <p>Retrieves an item of metadata that has previously been stored using the <code>scraperwiki.sqlite.save_var</code> function.</p>
				<dl>
	              <dt><code>key</code></dt>
	              <dd>The unique identifier that was used when storing the data.</dd>
	              <dt><code>default</code> (optional)</dt>
	              <dd>The value to be returned if no value is found to be associated with this key. Defaults to <code>None</code>.</dd>
	            </dl></td>
			    </tr>
			    </table>
				<h4>Example</h4>
				<div class="example"><code>
				latest_message = scraperwiki.sqlite.get_var('latest_message', default='No message yet')<br/>
				latest_message = 'Scraper input'<br/>
				scraperwiki.sqlite.save_var('latest_message',latest_message)
                </code></div>
                </div>



        <div class="section">
	    <h2><span id="geocoding"></span>Geocoding data</h2>
	    <p>ScraperWiki provides three useful geocoding functions: extract the postcode from an address;
	    convert postcode to lat/lng; and convert easting/northing to lat/lng.</p> 
	    <h4>Method</h4>
	    <table class="api_detail example_table">
	    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
		<tr><td><code>scraperwiki.geo.extract_gb_postcode
		      <br/>&nbsp;(string)</code></td><td>String</td>
		    <td><p>Attempts to extract a UK postcode from a given string.</p>
            <dl>
              <dt><code>string</code></dt>
              <dd>String. The string (e.g. an address) that will be searched for a postcode. Returns 
                 <code>False</code> if no postcode can be found. </dd>
            </dl>
            </td>
		    </tr>
		<tr><td><code>scraperwiki.geo.gb_postcode_to_latlng
			      <br/>&nbsp;(postcode)</code></td><td>List</td>
		    <td>
		    <p>Find the lat/lng for a given postcode.</p>
	        <dl>
	          <dt><code>postcode</code></dt>
	          <dd>
	            String. The postcode to geocode. 
	            You can supply the returned value as the <code>latlng</code> field in 
	            <a href="#saving">scraperwiki.datastore.save</a>.
	          </dd>
	        </dl>
		    </td>
		    </tr>
		<tr><td><code>scraperwiki.geo.os_easting_northing_to_latlng
			      <br/>&nbsp;(easting, northing)</code></td><td>List</td>
		    <td>
		    <p>
              Convert <a href="http://en.wikipedia.org/wiki/British_national_grid_reference_system">OSGB</a>
              easting/northing to lat/lng. 
            </p>
            <dl>
              <dt><code>easting</code></dt>
              <dd>Integer. The easting of the point to be converted.</dd>
              <dt><code>northing</code></dt>
              <dd>Integer. The northing of the point to be converted.</dd>
            </dl>
		    </td>
		    </tr>	
	    </table>
		<h4>Example</h4>
		<div class="example"><code>
		postcode = scraperwiki.geo.extract_gb_postcode("10 Downing Street, London, SW1A 2AA")<br/>
		latlng = scraperwiki.geo.gb_postcode_to_latlng("SW1A 2AA")<br/>
		oslatlng = scraperwiki.geo.os_easting_northing_to_latlng(528882,179839)
		</code></div>
        </div>


<hr style="margin-top:40px"/>
<div class="staff">
<h2>Deprecated functions</h2>
        <div class="section">
            <h3><span id="saving"></span>scraperwiki.datastore.save</h3>
            <p>Save your data to the ScraperWiki datastore. You can then download it, 
            or retrieve it later using the API. </p>
			    <h4>Method</h4>
			    <table class="api_detail example_table">
			    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
				<tr><td><code>scraperwiki.datastore.save<br/>
				    &nbsp;(unique_keys, data,<br/>
				    &nbsp;[date=],<br/>
				    &nbsp;[latlng=]) </code></td>
				<td>None</td>
			    <td>
			    <p>Stores a new record in the database, or replaces an existing
                record which has the same unique keys. The data parameter is a
                dictionary containing all the attributes of your record and their
                values.
                </p>
                <dl>
                    <dt><code>unique_keys</code></dt>
                        <dd>Array. Specify the attributes in
                        your record which are together unique, e.g. <code>['dog_id']</code> or
                        <code>['cattery', 'cat_number']</code>. Any existing data with
                        the same values for those attributes as your new data
                        will be overwritten. Must contain at least one element.
                        </dd>
                    <dt><code>data</code></dt>
                        <dd>Dictionary. The data in your record. Values are converted to strings, e.g.
                        in the example given, 
                        the integer <code>9812301</code> will be stored as a string. A null value
                        (e.g. <code>None</code> in Python) is stored as an empty string.
                        For string values, pass in either a Unicode string or a byte string encoded in UTF-8.
                        As a special case, if you use a key called <code>'date'</code> or <code>'latlng'</code>,
                        then it will be treated as if it were one of the optional arguments described below.
                        </dd>
                    <dt><code>date</code> (optional)</dt>
                        <dd>Special date/time field. You can later retrieve
                        information according to this date using the <a href="{% url api:scraper_getdatabydate %}">getDataByDate</a>
                        function in the external API. You can also store dates
                        in the data field, but you should put the most important date in this field.
                        The date should be a date/time object (e.g. <code>datetime.datetime.now() - datetime.timedelta(weeks=2)</code>).
                        </dd>
                    <dt><code>latlng</code> (optional)</dt>
                        <dd>Optionally, a location. This is a pair containing
                        latitude and longitude coordinates, e.g. <code>(-2.983333, 53.4)</code>. The coordinates should
                        be in the WGS84 projection. You can later retrieve
                        information according to this location using the <a href="{% url api:scraper_getdatabylocation %}">getDataByLocation</a>.
                        Scrapers for which latitude and longitude coordinates have been added will have a map showing the points displayed
                        on the its page on the site.
                        </dd>
                </dl>
			    </td>
			    </tr>
			    </table>
				<h4>Example</h4>
				<div class="example"><code>
			    data = {}<br/>
			    data['name'] = 'Fluffles'<br/>
			    data['breed'] = 'Alsatian'<br/>
			    crufts_date = datetime.datetime(2003, 8, 4, 12, 30, 45)<br/>
			    scraperwiki.datastore.save(['name'], data, crufts_date, latlng=(-2.983333, 53.4))
				</code></div>
            </div>

        <div class="section">
	    <h2><span id="managing"></span>Managing scrapers with metadata</h2> 
            <p>
              Scrapers can store and retrieve metadata that is persisted between
			runs. This can be used internally by the scraper; to set the order in which 
			data columns are displayed; or to hide particular columns in the data table.
            </p>
			    <h4>Method</h4>
			    <table class="api_detail example_table">
			    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
			<tr><td><code>scraperwiki.metadata.save<br/>&nbsp;(key, value)</code></td><td>String</td>
			    <td>
			 <p>Stores an item of metadata that can later be retrieved using the <code>scraperwiki.metadata.get</code> function.</p>
			    <dl>
	              <dt><code>key</code></dt>
	              <dd>
	                This is a unique identifier for the data being stored. Multiple save requests
	                with the same key will overwrite any existing value associated with the key.
	              </dd>
	              <dt><code>value</code></dt>
	              <dd>The value to be stored.</dd>
	            </dl></td></tr>
	            <tr>
				<td><code>scraperwiki.metadata.get<br/>&nbsp;(key, [default=])</code></td><td>String</td>
				 <td>
				 <p>Retrieves an item of metadata that has previously been stored using the <code>scraperwiki.metadata.save</code> function.</p>
				<dl>
	              <dt><code>key</code></dt>
	              <dd>The unique identifier that was used when storing the data.</dd>
	              <dt><code>default</code> (optional)</dt>
	              <dd>The value to be returned if no value is found to be associated with this key. Defaults to <code>None</code>.</dd>
	            </dl></td>
			    </tr>
			    </table>
				<h4>Example</h4>
				<div class="example"><code>
				latest_message = scraperwiki.metadata.get('latest_message', default='No message yet')<br/>
				latest_message = 'Scraper input'<br/>
				scraperwiki.metadata.save('latest_message',latest_message)
                </code></div>
                </div>

            <h3>Special Keys</h3>
            <p>
              You can store any information in metadata. However, special keys can be used to set
             column ordering in the data table, or hide columns altogether. These include:
            </p>
            <table class='data'>
              <tr><th>Key</th><th>Outcome</th></tr>
              <tr>
                <td>data_columns</td>
                <td>
                  Store a list of column names here to specify the order in which columns are displayed in the ScraperWiki data table.
                </td>
              </tr>
            </table>
			<h4>Example</h4>
			<div class="example"><code>
			scraperwiki.metadata.save('data_columns', ['date', 'name', 'breed', 'latlng'])
			</code></div>
        </div>

</div>

<div class="content col right">
<h4>Core ScraperWiki Python functions</h4>
<h5>Scraping</h5>
<ul>
  <li><a href="#scraping">scraperwiki.scrape</a></li>
</ul>
<h5>SQL datastore</h5>
<ul>
  <li><a href="#sql">scraperwiki.sqlite.save</a></li>
  <li><a href="#sql">scraperwiki.sqlite.execute</a></li>
  <li><a href="#sql">scraperwiki.sqlite.select</a></li>
  <li><a href="#sql">scraperwiki.sqlite.commit</a></li>
  <li><a href="#sql">scraperwiki.sqlite.attach</a></li>
  <li><a href="#variables">scraperwiki.sqlite.save_var</a></li>
  <li><a href="#variables">scraperwiki.metadata.get_var</a></li>
</ul>
<h5>Geo</h5>
<ul>
  <li><a href="#geocoding">scraperwiki.geo.extract_gb_postcode</a></li>
  <li><a href="#geocoding">scraperwiki.geo.gb_postcode_to_latlng</a></li>
  <li><a href="#geocoding">scraperwiki.geo.os_easting_northing_to_latlng</a></li>
</ul>
<h5>Deprecated</h5>
<ul>
  <li><a href="#saving">scraperwiki.datastore.save</a></li>
  <li><a href="#managing">scraperwiki.metadata.save</a></li>
  <li><a href="#managing">scraperwiki.metadata.get</a></li>
</ul> 
</div>

