<div class="documentationpage">

<p>In addition to all the standard Python libraries for downloading and parsing pages from the web, 
ScraperWiki provides the following built in library which you can access by including 
<code>import scraperwiki</code> at the top of your code.</p>

<p>The source code implementation of these functions can be found 
<a href="https://bitbucket.org/ScraperWiki/scraperwiki/src/efd21ea1c875/scraperlibs/">here</a>.</p>


<h3><span id="sql"></span>The SQLite datastore</h3>
<p>ScraperWiki provides a fully-fledged SQLite database for each scraper which you can save to.  
You can read the data back that has been committed by other scrapers, or extract it through 
the API</p>

<dl>
<dt>scraperwiki.<strong>sqlite.save</strong>(unique_keys, data[, table_name="swdata", verbose=2])</dt>
    <dd>Saves a data record into the datastore into the table given by table_name.  <br/>
        data is a dict object, unique_keys is a subset of data.keys() which determins 
        when a record is over-written.<br/>
        For large numbers of records data can be a list of dicts.
    </dd>

<dt>scraperwiki.<strong>sqlite.attach</strong>(name[, asname])</dt>
    <dd>Attaches to the datastore of another scraper with the given name.<br/>
        asname is an optional alias for the attached datastore.
    </dd>

<dt>scraperwiki.<strong>sqlite.select</strong>(val1[, val2], verbose=1)</dt>
    <dd>Executes a select command on the datastore, eg select("* from swdata limit 10")<br/>
        Returns a list of dicts that have been selected<br/>
        val2 is an optional list of parameters if the select command contains ?s
    </dd>

<dt>scraperwiki.<strong>sqlite.execute</strong>(val1[, val2, verbose=1])</dt>
    <dd>Executes any arbitrary sqlite command (except attach), eg create, delete, insert or drop<br/>
        val2 is an optional list of parameters if the select command contains question marks
    </dd>

<dt>scraperwiki.<strong>sqlite.commit</strong>()</dt>
    <dd>Commits to the file after a series of execute commands.  (sqlite.save auto-commits).
    </dd>

<dt>scraperwiki.<strong>sqlite.save_var</strong>(key, value)</dt>
    <dd>Saves an arbitrary single-value into a table called "swvariables". 
        Used to make scrapers able to continue after an interruption.
    </dd>

<dt>scraperwiki.<strong>sqlite.get_var</strong>(key[, default])</dt>
    <dd>Retrieves a single value that was saved by save_var.
    </dd>

</dl>




<h3>Geocoding functions</h3>
<p>Some installed functions to help you transform between different coordinate systems.</p> 

<dl>
<dt>scraperwiki.<strong>geo.extract_gb_postcode</strong>(string)</dt>
    <dd>Attempts to extract a UK postcode from a given string.
    </dd>

<dt>scraperwiki.<strong>geo.gb_postcode_to_latlng</strong>(postcode)</dt>
    <dd>Returns a WGS84 (lat, lng) pair for the central location of a UK postcode.
    </dd>

<dt>scraperwiki.<strong>geo.os_easting_northing_to_latlng</strong>(easting, northing)</dt>
    <dd>Converts a <a href="http://en.wikipedia.org/wiki/British_national_grid_reference_system">OSGB</a> 
        grid reference to a WGS84 (lat, lng) pair.
    </dd>

</dl>


<h3>Misc data format functions</h3>
<dl>

<dt>scraperwiki.<strong>pdftoxml</strong>(pdfdata)</dt>
    <dd>Convert a byte string containing a PDF file into an XML file containing the coordinates 
        and font of each text string.  <br/>
        Refer to 
        <a href="http://scraperwiki.com/scrapers/new/python?template=advanced-scraping-pdfs">the example</a> 
        for more details.
    </dd>

<dt>scraperwiki.<strong>utils.httpresponseheader</strong>("Content-Type", "image/PNG")</dt>
    <dd>Set the content-type header to something other than HTML when using a Scraperwiki "view" 
        (a simplified CGI script).
    </dd>

<dt>scraperwiki.<strong>utils.GET</strong>()</dt>
    <dd>The query_string map when using a Scraperwiki "view" <em>(not yet deployed)</em>.
    </dd>

</dl>

<h3>Deprecated functions</h3>

<dl>
<dt>scraperwiki.<strong>datastore.save</strong>(unique_keys, data[, date, latlng])</dt>
    <dd>Precursor for the sqlite.save() function</dd>

<dt>scraperwiki.<strong>metadata.save</strong>(key, value)</dt>
    <dd>Now user sqlite.save_var(key, value)</dd>

<dt>scraperwiki.<strong>metadata.get</strong>(key[, default])</dt>
    <dd>Now user sqlite.get_var(key)</dd>

</dl>

</div>

