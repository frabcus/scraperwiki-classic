<div class="content col left">
<h3>Scraperwiki Python documentation</h3>

<p>In your scrapers, you can use ScraperWiki's in-built functions for retrieving, geocoding and saving data. 
Just add <code>import scraperwiki</code> at the top of your scraper.</p>

<div class="section">
<h3><span id="scraping"></span>scraperwiki.scrape</h3>
<p>The most basic function. Downloads a web page, and returns you the HTML as a string.</p>
    <h4>Method</h4>
    <table class="api_detail example_table">
    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
<tr><td><code>scraperwiki.scrape<br/>&nbsp;(url, [params=], )</code></td><td>String</td>
    <td><dl>
    <dt><code>url</code></dt>
        <dd>String. The address of the web page, e.g. <code>http://www.google.co.uk</code>
        </dd>
    <dt><code>params</code> (optional)</dt>
<dd>Dictionary. If present, sends a POST request, with the parameters specified in the dictionary. 
        </dd>
    </dl></td>
    </tr>
    </table>
	<h4>Example</h4>
	<div class="example"><code>
	import scraperwiki<br/>
	html = scraperwiki.scrape('http://www.google.co.uk') # GET request.<br/>
	html = scraperwiki.scrape('http://www.google.co.uk', { 'key' : value })	# POST request.
	</code></div>
</div>

        <div class="section">
            <h3><span id="saving"></span>scraperwiki.datastore.save</h3>
            <p>Save your data to the ScraperWiki datastore. You can then download it, 
               or retrieve it later using the API.</p>
			    <h4>Method</h4>
			    <table class="api_detail example_table">
			    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
				<tr><td><code>scraperwiki.datastore.save<br/>
				    &nbsp;(unique_keys, data,<br/>
				    &nbsp;[date=],<br/>
				    &nbsp;[latlng=]) </code></td>
				<td>None</td>
			    <td>
			    <p>Stores a new record in the database, or replaces an existing
                record which has the same unique keys. The data parameter is a
                dictionary containing all the attributes of your record and their
                values.
                </p>
                <dl>
                    <dt><code>unique_keys</code></dt>
                        <dd>Array. Specify the attributes in
                        your record which are together unique, e.g. <code>['dog_id']</code> or
                        <code>['cattery', 'cat_number']</code>. Any existing data with
                        the same values for those attributes as your new data
                        will be overwritten. Must contain at least one element.
                        </dd>
                    <dt><code>data</code></dt>
                        <dd>Dictionary. The data in your record. Values are converted to strings, e.g.
                        in the example given, 
                        the integer <code>9812301</code> will be stored as a string. A null value
                        (e.g. <code>None</code> in Python) is stored as an empty string.
                        As a special case, if you use a key called <code>'date'</code> or <code>'latlng'</code>,
                        then it will be treated as if it were one of the optional arguments described below.
                        </dd>
                    <dt><code>date</code> (optional)</dt>
                        <dd>Special date/time field. You can later retrieve
                        information according to this date using the <a href="{% url api:scraper_getdatabydate %}">getDataByDate</a>
                        function in the external API. You can also store dates
                        in the data field, but you should put the most important date in this field.
                        The date should be a date/time object (e.g. <code>datetime.datetime.now() - datetime.timedelta(weeks=2)</code>).
                        </dd>
                    <dt><code>latlng</code> (optional)</dt>
                        <dd>Optionally, a location. This is a pair containing
                        latitude and longitude coordinates, e.g. <code>(-2.983333, 53.4)</code>. The coordinates should
                        be in the WGS84 projection. You can later retrieve
                        information according to this location using the <a href="{% url api:scraper_getdatabylocation %}">getDataByLocation</a>.
                        Scrapers for which latitude and longitude coordinates have been added will have a map showing the points displayed
                        on the its page on the site.
                        </dd>
                </dl>
			    </td>
			    </tr>
			    </table>
				<h4>Example</h4>
				<div class="example"><code>
			    data = {}<br/>
			    data['name'] = 'Fluffles'<br/>
			    data['breed'] = 'Alsatian'<br/>
			    crufts_date = datetime.datetime(2003, 8, 4, 12, 30, 45)<br/>
			    scraperwiki.datastore.save(['name'], data, crufts_date, latlng=(-2.983333, 53.4))
				</code></div>
            </div>

	        <div class="section">
	            <h3><span id="sql"></span>The SQLite datastore</h3>
	            <p>In addition to the standard datastore, ScraperWiki also offers access
	                to a fully-fledged SQLite database that persists between runs.</p>
	            <p>You can save data to the SQLite database in exactly the same way as the 
	            ScraperWiki datastore using 
	            <code>scraperwiki.sqlite.save</code> but you can also carry out more advanced SQL commands
	            directly (using <code>scraperwiki.sqlite.execute</code> and 
	            <code>scraperwiki.sqlite.select</code>).</p>
	            <p>When you save to the SQLite database, it becomes the default datastore for the scraper. 
	            You can retrieve data later from within other scrapers, or via the API.</p>
				    <h4>Method</h4>
				    <table class="api_detail example_table">
				    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
					<tr><td><code>scraperwiki.sqlite.save<br/>
					    &nbsp;(unique_keys, data,<br/>
					    &nbsp;[date=],<br/>
					    &nbsp;[latlng=]) </code></td>
					<td>None</td>
				    <td>
				    <p>Stores a new record in the SQLite database, or replaces an existing
	                record which has the same unique keys. The data parameter is a
	                dictionary containing all the attributes of your record and their
	                values.
	                </p>
	                <dl>
	                    <dt><code>unique_keys</code></dt>
	                        <dd>Array. Specify the attributes in
	                        your record which are together unique, e.g. <code>['dog_id']</code> or
	                        <code>['cattery', 'cat_number']</code>. Any existing data with
	                        the same values for those attributes as your new data
	                        will be overwritten. Must contain at least one element.
	                        </dd>
	                    <dt><code>data</code></dt>
	                        <dd>Dictionary. The data in your record. Values are converted to strings, e.g.
	                        in the example given, 
	                        the integer <code>9812301</code> will be stored as a string. A null value
	                        (e.g. <code>None</code> in Python) is stored as an empty string.
	                        As a special case, if you use a key called <code>'date'</code> or <code>'latlng'</code>,
	                        then it will be treated as if it were one of the optional arguments described below.
	                        </dd>
	                    <dt><code>date</code> (optional)</dt>
	                        <dd>Special date/time field. You can later retrieve
	                        information according to this date using the <a href="{% url api:scraper_getdatabydate %}">getDataByDate</a>
	                        function in the external API. You can also store dates
	                        in the data field, but you should put the most important date in this field.
	                        The date should be a date/time object (e.g. <code>datetime.datetime.now() - datetime.timedelta(weeks=2)</code>).
	                        </dd>
	                    <dt><code>latlng</code> (optional)</dt>
	                        <dd>Optionally, a location. This is a pair containing
	                        latitude and longitude coordinates, e.g. <code>(-2.983333, 53.4)</code>. The coordinates should
	                        be in the WGS84 projection. You can later retrieve
	                        information according to this location using the <a href="{% url api:scraper_getdatabylocation %}">getDataByLocation</a>.
	                        Scrapers for which latitude and longitude coordinates have been added will have a map showing the points displayed
	                        on the its page on the site.
	                        </dd>
	                </dl>
				    </td>
				    </tr>
					<tr><td><code>scraperwiki.sqlite.execute<br/>
					    &nbsp;(command)</code></td>
					<td>None</td>
				    <td>
				    <p>Executes an SQLite command.
	                </p>
	                <dl>
	                    <dt><code>command</code></dt>
	                        <dd>String. An SQLite command. See the 
                          <a href="http://www.sqlite.org/docs.html">SQLite documentation</a> for 
                          command syntax.
	                        </dd>
	                </dl>
				    </td>
				    </tr>
					<tr><td><code>scraperwiki.sqlite.select<br/>
					    &nbsp;(command)</code></td>
					<td>List</td>
				    <td>
				    <p>Selects from the SQLite database.
	                </p>
	                <dl>
	                    <dt><code>command</code></dt>
	                        <dd>String. An SQLite select expression.  See the 
                          <a href="http://www.sqlite.org/docs.html">SQLite documentation</a> for 
                          command syntax.
	                        </dd>
	                </dl>
				    </td>
				    </tr>
					<tr><td><code>scraperwiki.sqlite.commit<br/>
					    &nbsp;()</code></td>
					<td>None</td>
				    <td>
				    <p>Commits your SQLite changes. Required to make your changes persist between runs.
	                </p>
				    </td>
				    </tr>				
				    </table>
					<h4>Example</h4>
					<div class="example"><code>
					scraperwiki.sqlite.save(['name'], data, crufts_date, latlng=(-2.983333, 53.4))<br/>
					person_fields = ["booking_id INTEGER PRIMARY KEY","name TEXT"]<br/>
					scraperwiki.sqlite.execute("create table if not exists person (%s)" \<br/>
					&nbsp;&nbsp;% ",".join(person_fields))<br/>
					scraperwiki.sqlite.execute("INSERT into 'person' values(?,?);",(100, 'Dave'))<br/>
					scraperwiki.sqlite.commit()<br/>
					results = scraperwiki.sqlite.select("booking_id from person where name='Dave'")
					</code></div>
	            </div>


        <div class="section">
	    <h2><span id="geocoding"></span>Geocoding data</h2>
	    <p>ScraperWiki provides three useful geocoding functions: extract the postcode from an address;
	    convert postcode to lat/lng; and convert easting/northing to lat/lng.</p> 
	    <h4>Method</h4>
	    <table class="api_detail example_table">
	    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
		<tr><td><code>scraperwiki.geo.extract_gb_postcode
		      <br/>&nbsp;(string)</code></td><td>String</td>
		    <td><p>Attempts to extract a UK postcode from a given string.</p>
            <dl>
              <dt><code>string</code></dt>
              <dd>String. The string (e.g. an address) that will be searched for a postcode. Returns 
                 <code>False</code> if no postcode can be found. </dd>
            </dl>
            </td>
		    </tr>
		<tr><td><code>scraperwiki.geo.gb_postcode_to_latlng
			      <br/>&nbsp;(postcode)</code></td><td>List</td>
		    <td>
		    <p>Find the lat/lng for a given postcode.</p>
	        <dl>
	          <dt><code>postcode</code></dt>
	          <dd>
	            String. The postcode to geocode. 
	            You can supply the returned value as the <code>latlng</code> field in 
	            <a href="#saving">scraperwiki.datastore.save</a>.
	          </dd>
	        </dl>
		    </td>
		    </tr>
		<tr><td><code>scraperwiki.geo.os_easting_northing_to_latlng
			      <br/>&nbsp;(easting, northing)</code></td><td>List</td>
		    <td>
		    <p>
              Convert <a href="http://en.wikipedia.org/wiki/British_national_grid_reference_system">OSGB</a>
              easting/northing to lat/lng. 
            </p>
            <dl>
              <dt><code>easting</code></dt>
              <dd>Integer. The easting of the point to be converted.</dd>
              <dt><code>northing</code></dt>
              <dd>Integer. The northing of the point to be converted.</dd>
            </dl>
		    </td>
		    </tr>	
	    </table>
		<h4>Example</h4>
		<div class="example"><code>
		postcode = scraperwiki.geo.extract_gb_postcode("10 Downing Street, London, SW1A 2AA")<br/>
		latlng = scraperwiki.geo.gb_postcode_to_latlng("SW1A 2AA")<br/>
		oslatlng = scraperwiki.geo.os_easting_northing_to_latlng(528882,179839)
		</code></div>
        </div>

        <div class="section">
	    <h2><span id="managing"></span>Managing scrapers with metadata</h2> 
            <p>
              Scrapers can store and retrieve metadata that is persisted between
			runs. This can be used internally by the scraper; to set the order in which 
			data columns are displayed; or to hide particular columns in the data table.
            </p>
			    <h4>Method</h4>
			    <table class="api_detail example_table">
			    <tr><th width="30%">Method</th><th width="20%">Returned value</th><th>Description</th></tr>
			<tr><td><code>scraperwiki.metadata.save<br/>&nbsp;(key, value)</code></td><td>String</td>
			    <td>
			 <p>Stores an item of metadata that can later be retrieved using the <code>scraperwiki.metadata.get</code> function.</p>
			    <dl>
	              <dt><code>key</code></dt>
	              <dd>
	                This is a unique identifier for the data being stored. Multiple save requests
	                with the same key will overwrite any existing value associated with the key.
	              </dd>
	              <dt><code>value</code></dt>
	              <dd>The value to be stored.</dd>
	            </dl></td></tr>
	            <tr>
				<td><code>scraperwiki.metadata.get<br/>&nbsp;(key, [default=])</code></td><td>String</td>
				 <td>
				 <p>Retrieves an item of metadata that has previously been stored using the <code>scraperwiki.metadata.save</code> function.</p>
				<dl>
	              <dt><code>key</code></dt>
	              <dd>The unique identifier that was used when storing the data.</dd>
	              <dt><code>default</code> (optional)</dt>
	              <dd>The value to be returned if no value is found to be associated with this key. Defaults to <code>None</code>.</dd>
	            </dl></td>
			    </tr>
			    </table>
				<h4>Example</h4>
				<div class="example"><code>
				latest_message = scraperwiki.metadata.get('latest_message', default='No message yet')<br/>
				latest_message = 'Scraper input'<br/>
				scraperwiki.metadata.save('latest_message',latest_message)
                </code></div>

            <h3>Special Keys</h3>
            <p>
              You can store any information in metadata. However, special keys can be used to set
             column ordering in the data table, or hide columns altogether. These include:
            </p>
            <table class='data'>
              <tr><th>Key</th><th>Outcome</th></tr>
              <tr>
                <td>data_columns</td>
                <td>
                  Store a list of column names here to specify the order in which columns are displayed in the ScraperWiki data table.
                </td>
              </tr>
            </table>
			<h4>Example</h4>
			<div class="example"><code>
			scraperwiki.metadata.save('data_columns', ['date', 'name', 'breed', 'latlng'])
			</code></div>
        </div>
</div>

<div class="content col right">
<h4>Core ScraperWiki Python functions</h4>
<h5>Scraping</h5>
<ul>
  <li><a href="#scraping">scraperwiki.scrape</a></li>
</ul>
<h5>Datastore</h5>
<ul>
  <li><a href="#saving">scraperwiki.datastore.save</a></li>
</ul>
<h5>SQL datastore</h5>
<ul>
  <li><a href="#sql">scraperwiki.sqlite.save</a></li>
  <li><a href="#sql">scraperwiki.sqlite.execute</a></li>
  <li><a href="#sql">scraperwiki.sqlite.select</a></li>
  <li><a href="#sql">scraperwiki.sqlite.commit</a></li>
</ul>
<h5>Geo</h5>
<ul>
  <li><a href="#geocoding">scraperwiki.geo.extract_gb_postcode</a></li>
  <li><a href="#geocoding">scraperwiki.geo.gb_postcode_to_latlng</a></li>
  <li><a href="#geocoding">scraperwiki.geo.os_easting_northing_to_latlng</a></li>
</ul>
<h5>Metadata</h5>
<ul>
  <li><a href="#managing">scraperwiki.metadata.save</a></li>
  <li><a href="#managing">scraperwiki.metadata.get</a></li>
</ul> 
</div>

