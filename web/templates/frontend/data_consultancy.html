{% extends "frontend/base.html" %}

{% block title %}Data Consultancy{% endblock %}

{% block meta %}
    <meta name="description" content="ScraperWiki is the data hub for professional data scientists. Our Corporate Services enable you to tap into that thriving community so you always find the right person for the job. ScraperWiki also helps organise data across your enterprise, combining the flexibility of general purpose code with a rock-solid API and granular permissions over precisely who sees what." />
{% endblock %}

{% block header %}
    <h2><a href="/data_services/">Data Services</a> / <a href="{% url data_consultancy %}">Data Consultancy</a></h2>
{% endblock %}

{% block content %}

<div class="intro">
    <h3>Data Collection &amp; Curation Services</h3>
    <p>Nobody knows your data better than you. However sometimes you need to bring in outside skills to help. At ScraperWiki we have a worldwide pool of expert data scientists able to address any data problem.</p>
    <p>Most data projects go through four stages&hellip;</p>
</div>

<div class="aside" id="agricultural">
    <h4>Case Study</h4>
    <h3>Scraping agricultural prices for a B2B&nbsp;blue-chip</h3>
    <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent feugiat felis eu libero rhoncus a fermentum lectus bibendum. Aliquam erat volutpat. Nulla sed augue ipsum. Aenean semper mauris eget turpis volutpat hendrerit. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eget justo at elit iaculis viverra. Nullam non convallis est. Fusce quis ligula nec orci venenatis placerat. Aliquam ac felis orci, eu gravida tortor.</p>
    <p>Etiam mattis ultrices neque id tristique. Aliquam blandit aliquam fermentum. Curabitur varius accumsan vehicula. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Praesent cursus malesuada leo.</p>
</div>

<div class="step">
    <h3>Step 1: Extraction</h3>
    <p>ScraperWiki is great at really hard ETL (Extract, Transform and Load) tasks because we use the magic of programming. We can pull data together to create new value.</p>
    <ol>
        <li><strong>Public data.</strong> Gather together data from thousands of pages, from hundred of websites, to make one data set, that automatically updates itself every day or every hour.</li>
        <li><strong>Multiple PDFs.</strong> The black art of data extraction, we&apos;ve done it for your competitors, and we can do it for you.</li>
        <li><strong>Legacy systems.</strong> From older database systems, screen grabbing, arcane file formats &ndash; transforming data is our core expertise.</li>
        <li><strong>SaaS applications.</strong> We can gather and integrate data from cloud applications which you use, whether or not they have APIs.</li>
    </ol>
</div>

<div class="step">
    <h3>Step 2: Cleaning</h3>
    <p>Once data is extracted it very often requires some level of cleaning. This can be as simple as adjusting date and time formats, to full name checks in the process creating unique identifiers.</p>
    <p>At this stage in the process it&apos;s essential to have good communications between our data scientists and the client. We log the change process so that clients can clearly audit the transformation and ensure quality control.</p>
</div>

<div class="aside" id="legal">
    <h4>Case Study</h4>
    <h3>Another really persuasive heading goes&nbsp;here</h3>
    <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent feugiat felis eu libero rhoncus a fermentum lectus bibendum. Aliquam erat volutpat. Nulla sed augue ipsum. Aenean semper mauris eget turpis volutpat hendrerit. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eget justo at elit iaculis viverra. Nullam non convallis est. Fusce quis ligula nec orci venenatis placerat. Aliquam ac felis orci, eu gravida tortor.</p>
    <p>Etiam mattis ultrices neque id tristique. Aliquam blandit aliquam fermentum. Curabitur varius accumsan vehicula. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Praesent cursus malesuada leo.</p>
</div>

<div class="step">
    <h3>Step 3: Integration</h3>
    <p>Once your data is in a form that is useful to you, we can feed this into whatever system you need. You can always browse it as a spreadsheet, and your developers can access it via our flexible API.</p>
    <p>We can push the data into your web platform, to provide your subscribers or users with the most up-to-date and visually stimulating representation imaginable.</p>
    <p>We can also securely integrate with your systems in any way you like. From email, FTP and Dropbox to full private installs of ScraperWiki behind your firewall.</p>
</div>

<div class="step">
    <h3>Step 2: Visualisations and applications</h3>
    <p>It&apos;s not the data, but what you do with it, that matters. We can help you add value to your data by creating anything from basic graphs and maps, to full blown data applications which you can sell to your readers. And we do it all using web standards, so it&apos;ll work on notebooks, iPads and Androids.</p>
</div>

<div class="sla">
    <h3>Service Level Agreement</h3>
    <p>Get the service you expect, with an industry-standard SLA covering resilience, alerts and escalation. <a href="http://status.scraperwiki.com">Monitor our status</a> from your dashboard and receive automatic refunds if we fail to deliver.</p>
</div>

{% endblock %}
