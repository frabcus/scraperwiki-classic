{% extends "frontend/base.html" %}

{% block title %}Code documentation{% endblock %}
{% block content %}
    <div class="page_title">
        <h1>Code documentation</h1>
        <p>
        The following libraries and function calls are available to use in your screenscraper.
        They are all in a normal python module, called <code>scraperwiki</code>. For
        example, you can do <code>from scraperwiki import datastore</code>, and then
        call the function <code>datastore.save</code>.
        </p>
    </div>
    <div class="content programmer_api">
        <div class="section">
            <h2>Scraping</h2>
            <p>How to retrieve HTML pages from websites.</p>
            <h3>scraperwiki.scrape(url, [params=], )</h3>
            <p>Downloads a web page, and returns you the HTML. The page will appear in the "sources"
            pane in the scraper code editor, so you can easily view what was downloaded.
                </p>
                <dl>
                    <dt><code>url</code></dt>
                        <dd>The address of the web page, e.g. <code>http://www.parliament.uk/mpslordsandoffices/government_and_opposition/hmg.cfm</code>
                        </dd>
                    <dt><code>params</code> (optional)</dt>
                        <dd>If present, makes the HTTP request a POST request, such as if a form
                        had been submitted. <code>params</code> is a dictionary, whose keys are
                        the names of the fields being posted, and values are their values.
                        </dd>
                 </dl>
            <h3>Standard libraries</h3>
            <p>You can download pages using all the standard Python functions.
            The page will not appear in the "sources" pane).
            </p>
            <ul>
                <li><code><b>urllib2</b>, <b>urlparse</b></code>
                    Standard python libraries for opening urls. <a href="http://docs.python.org/library/urllib2.html" target="_blank">docs</a>
                </li>
                <li><code><b>BeautifulSoup</b></code>
                    For parsing html. <a href="http://www.crummy.com/software/BeautifulSoup/" target="_new">docs</a>
                </li>
                <li><code><strong>mechanize</strong></code>
                    For navigating form fillings. <a href="http://wwwsearch.sourceforge.net/mechanize/" target="_new">docs</a>
                </li>
                <li>
                    <code><strong>xlrd</strong></code>
                    For reading Excel files. <a href="http://www.lexicon.net/sjmachin/xlrd.htm" target="_new">docs</a>
                </li>
            </ul>
        </div>


        <div class="section">
            <h2>Data store</h2>
            <p>This is where you put the output from your scraper. It is a key-value database.
            This means that you do not have to specify a structure in advance, you just add
            the data and it stores it. You can read from the data store using the
            <a href="{% url help_code_documentation %}">external API</a>.</p>

            <div class="call">
                <h3>scraperwiki.datastore.save(unique_keys, data, [date=], [latlng=]) </h3>
                <p>Stores a new record in the database, or replaces an existing
                record which has the same unique keys. The data parameter is a
                dictionary containing all the attributes of your record and their
                values.
                </p>
                <dl>
                    <dt><code>unique_keys</code></dt>
                        <dd>An array containing the names of the attributes in
                        your record which are together unique. e.g. <code>['id']</code> or
                        <code>['cattery', 'cat_number']</code>. Any existing data with
                        the same values for those attributes as your new data
                        will be replaced. If there is no such existing data, or
                        if <code>unique_keys</code> is an empty array, 
                        it always creates a new record.
                        </dd>
                    <dt><code>data</code></dt>
                        <dd>A dictionary containing the data in your record.  The
                        keys are strings, and the values are converted to strings. e.g.
                        <code>{ 'name': 'Ruffles', 'dog_id': '9812301', 'breed': 'Alsation', 'weight': 28 }</code>,
                        the integer <code>weight</code> will be stored as a string. A null value
                        (e.g. <code>None</code> in Python) is stored as an empty string.
                        As a special case, if you use a key called <code>'date'</code> or <code>'latlng'</code>,
                        then it will be treated as if it were one of the optional arguments described below.
                        </dd>
                    <dt><code>date</code> (optional)</dt>
                        <dd>Optionally, a special date/time field. You can later retrieve
                        information according to this date using the <a href="{% url api:scraper_getdatabydate %}">getDataByDate</a>
                        function in the external API. You can also store dates
                        in the data field, but you should put the most important one in this field.
                        The date can be formatted as an ISO date in a string (<code>'2009-11-02'</code>), 
                        can include a time (<code>'2015-01-19 03:14:07'</code>) or can be
                        a date/time object from your language (e.g. <code>datetime.datetime.now() - datetime.timedelta(weeks=2)</code>).
                        </dd>
                    <dt><code>latlng</code> (optional)</dt>
                        <dd>Optionally, a location. This is a pair containing
                        latitude and longitude coordinates, e.g. <code>(-2.983333, 53.4)</code>. The coordinates should
                        be in the WGS84 projection. You can later retrieve
                        information according to this location using the <a href="{% url api:scraper_getdatabylocation %}">getDataByLocation</a>.
                        </dd>
                </dl>
                <p>Full example:
                <pre>
    data = {}
    data['name'] = 'Ruffles'
    data['dog_id'] = '9812301'
    data['breed'] = 'Alsation'
    scraperwiki.datastore.save([dog_id], data, date="2009-03-02", latlng=(-2.983333, 53.4))
                </pre>
            </div>
        </div>

        <div class="section">
	    <h2>Metadata</h2> 
            <p>
              Scrapers can store and retrieve items of metadata that are persisted between
              runs. These can either be used internally by the scraper or as a means of
              communicating information about the scraper (such as the url of a google chart
              made from the scraped data) to the site.
            </p>
            <h3>scraperwiki.metadata.put(key, value)</h3>
            <p>Stores an item of metadata that can later be retrieved using the <code>scraperwiki.metadata.get</code> function.</p>
            <dl>
              <dt><code>key</code></dt>
              <dd>
                This is a unique identifier for the data being stored. Multiple put requests
                with the same key will overwrite any existing value associated with the key.
              </dd>
              <dt><code>value</code></dt>
              <dd>The value to be stored.</dd>
            </dl>
            <h3>scraperwiki.metadata.get(key, [default=])</h3>
            <p>Retrieves an item of metadata that has previously been stored using the <code>scraperwiki.metadata.put</code> function.</p>
            <dl>
              <dt><code>key</code></dt>
              <dd>The unique identifier that was used when storing the data.</dd>
              <dt><code>default</code> (optional)</dt>
              <dd>The value to be returned if no value is found to be associated with this key. Defaults to <code>None</code>.</dd>
            </dl>
            <h3>Special Keys</h3>
            <p>
              As described above storing metadata may be used as a means of communicating
              information about the scraper to the ScraperWiki site. Below is a table of keys
              that may be used for this purpose and the intended outcome.
            </p>
            <table class='data'>
              <tr><th>Key</th><th>Outcome</th></tr>
              <tr>
                <td>chart</td>
                <td>
                  If a valid Google Charts url (starting with
                  http://chart.apis.google.com/chart?) is associated with this key then the chart
                  will be displayed on the scraper overview page.
                </td>
              </tr>
            </table>
        </div>

        <div class="section">
            <h2>Debugging</h2>
            <h3>print</h3>
            <p>To log what is happening, or as a general purpose tool to help
            debugging, use your language's standard print command. The output will appear
            in the "console" pane in the scraper code editor.
            </p>
        </div>
     </div>
{% endblock %}
