{% extends "frontend/base.html" %}

{% block title %}Code documentation{% endblock %}
{% block content %}
    <div class="page_title">
        <h1>Code documentation</h1>
        <p>
        The following libraries and function calls are available to use in your screenscraper.
        They are all in a normal python module, called <code>scraperwiki</code>. For
        example, you can do <code>from scraperwiki import datastore</code>, and then
        call the function <code>datastore.save</code>.
        </p>
    </div>
    <div class="content programmer_api">
        <div class="section">
            <h2>Scraping</h2>
            <p>How to retrieve HTML pages from websites.</p>
            <h3>scraperwiki.scrape(url, [params=], )</h3>
            <p>Downloads a web page, and returns you the HTML. The page will appear in the "sources"
            pane in the scraper code editor, so you can easily view what was downloaded.
                </p>
                <dl>
                    <dt><code>url</code></dt>
                        <dd>The address of the web page, e.g. <code>http://www.parliament.uk/mpslordsandoffices/government_and_opposition/hmg.cfm</code>
                        </dd>
                    <dt><code>params</code> (optional)</dt>
                        <dd>If present, makes the HTTP request a POST request, such as if a form
                        had been submitted. <code>params</code> is a dictionary, whose keys are
                        the names of the fields being posted, and values are their values.
                        </dd>
                 </dl>
            <h3>Standard libraries</h3>
            <p>You can download pages using all the standard Python functions.
            The page will not appear in the "sources" pane.
            </p>
            <ul>
                <li><code><b>urllib2</b>, <b>urlparse</b></code>
                    Standard python libraries for opening urls. <a href="http://docs.python.org/library/urllib2.html" target="_blank">docs</a>
                </li>
                <li><code><b>BeautifulSoup</b></code>
                    For parsing html. <a href="http://www.crummy.com/software/BeautifulSoup/" target="_new">docs</a>
                </li>
                <li><code><strong>mechanize</strong></code>
                    For navigating form fillings. <a href="http://wwwsearch.sourceforge.net/mechanize/" target="_new">docs</a>
                </li>
                <li><code><strong>xlrd</strong></code>
                    For reading Excel files. <a href="http://www.lexicon.net/sjmachin/xlrd.htm" target="_new">docs</a>
                </li>
                <li><code><strong>Python Google Chart</strong></code>
                    For generating charts using the Google Chart API. <a href="http://pygooglechart.slowchop.com/" target="_new">docs</a>
                </li>
            </ul>
        </div>

        <div class="section">
            <h2>Data store</h2>
            <p>This is where you put the output from your scraper. It is a key-value database.
            This means that you do not have to specify a structure in advance, you just add
            the data and it stores it. You can read from the data store using the
            <a href="{% url help_code_documentation %}">external API</a>.</p>

            <div class="call">
                <h3>scraperwiki.datastore.save(unique_keys, data, [date=], [latlng=]) </h3>
                <p>Stores a new record in the database, or replaces an existing
                record which has the same unique keys. The data parameter is a
                dictionary containing all the attributes of your record and their
                values.
                </p>
                <dl>
                    <dt><code>unique_keys</code></dt>
                        <dd>An array containing the names of the attributes in
                        your record which are together unique. e.g. <code>['id']</code> or
                        <code>['cattery', 'cat_number']</code>. Any existing data with
                        the same values for those attributes as your new data
                        will be replaced. If there is no such existing data, or
                        if <code>unique_keys</code> is an empty array, 
                        it always creates a new record.
                        </dd>
                    <dt><code>data</code></dt>
                        <dd>A dictionary containing the data in your record.  The
                        keys are strings, and the values are converted to strings. e.g.
                        <code>{ 'name': 'Ruffles', 'dog_id': '9812301', 'breed': 'Alsation', 'weight': 28 }</code>,
                        the integer <code>weight</code> will be stored as a string. A null value
                        (e.g. <code>None</code> in Python) is stored as an empty string.
                        As a special case, if you use a key called <code>'date'</code> or <code>'latlng'</code>,
                        then it will be treated as if it were one of the optional arguments described below.
                        </dd>
                    <dt><code>date</code> (optional)</dt>
                        <dd>Optionally, a special date/time field. You can later retrieve
                        information according to this date using the <a href="{% url api:scraper_getdatabydate %}">getDataByDate</a>
                        function in the external API. You can also store dates
                        in the data field, but you should put the most important one in this field.
                        The date can be formatted as an ISO date in a string (<code>'2009-11-02'</code>), 
                        can include a time (<code>'2015-01-19 03:14:07'</code>) or can be
                        a date/time object from your language (e.g. <code>datetime.datetime.now() - datetime.timedelta(weeks=2)</code>).
                        </dd>
                    <dt><code>latlng</code> (optional)</dt>
                        <dd>Optionally, a location. This is a pair containing
                        latitude and longitude coordinates, e.g. <code>(-2.983333, 53.4)</code>. The coordinates should
                        be in the WGS84 projection. You can later retrieve
                        information according to this location using the <a href="{% url api:scraper_getdatabylocation %}">getDataByLocation</a>.
                        Scrapers for which latitude and longitude coordinates have been added will have a map showing the points displayed
                        on the its page on the site.
                        </dd>
                </dl>
                <p>Full example:
                <pre>
    data = {}
    data['name'] = 'Ruffles'
    data['dog_id'] = '9812301'
    data['breed'] = 'Alsation'
    scraperwiki.datastore.save([dog_id], data, date="2009-03-02", latlng=(-2.983333, 53.4))
                </pre>
            </div>
        </div>

        <div class="section">
	    <h2>Geographic Data</h2>
            <p>There are a number of functions provided to help with processing geographic data.</p>
            <h3>scraperwiki.geo.gb_postcode_to_latlng(postcode)</h3>
            <p>Find the latitude and longitude for a given postcode.</p>
            <dl>
              <dt><code>postcode</code></dt>
              <dd>
                The postcode for which the latitude and longitude will be returned. This
                function is typically used for obtaining a value to send to the
                <code>scraperwiki.datastore.save</code> function to associate a geographic
                location with the record.
              </dd>
            </dl>
            <h3>scraperwiki.geo.os_easting_northing_to_latlng(easting, northing)</h3>
            <p>
              Convert <a href="http://en.wikipedia.org/wiki/British_national_grid_reference_system">OSGB</a>
              easting, northing coordinate to latitude and longitude. Again this function is typically used to
              obtain a value to send to the <code>scraperwiki.datastore.save</code> function.
            </p>
            <dl>
              <dt><code>easting</code></dt>
              <dd>The easting of the point to be converted.</dd>
              <dt><code>northing</code></dt>
              <dd>The northing of the point to be converted.</dd>
            </dl>
            <h3>scraperwiki.geo.extract_gb_postcode(string)</h3>
            <p>Attempts to extract a UK postcode from a given string.</p>
            <dl>
              <dt><code>string</code></dt>
              <dd>The string that will be searched for a postcode.</dd>
            </dl>
        </div>

        <div class="section">
	    <h2>Metadata</h2> 
            <p>
              Scrapers can store and retrieve items of metadata that are persisted between
              runs. These can either be used internally by the scraper or as a means of
              communicating information about the scraper (such as the url of a google chart
              made from the scraped data) to the site.
            </p>
            <h3>scraperwiki.metadata.save(key, value)</h3>
            <p>Stores an item of metadata that can later be retrieved using the <code>scraperwiki.metadata.get</code> function.</p>
            <dl>
              <dt><code>key</code></dt>
              <dd>
                This is a unique identifier for the data being stored. Multiple save requests
                with the same key will overwrite any existing value associated with the key.
              </dd>
              <dt><code>value</code></dt>
              <dd>The value to be stored.</dd>
            </dl>
            <h3>scraperwiki.metadata.get(key, [default=])</h3>
            <p>Retrieves an item of metadata that has previously been stored using the <code>scraperwiki.metadata.save</code> function.</p>
            <dl>
              <dt><code>key</code></dt>
              <dd>The unique identifier that was used when storing the data.</dd>
              <dt><code>default</code> (optional)</dt>
              <dd>The value to be returned if no value is found to be associated with this key. Defaults to <code>None</code>.</dd>
            </dl>
            <h3>Special Keys</h3>
            <p>
              As described above storing metadata may be used as a means of communicating
              information about the scraper to the ScraperWiki site. Below is a table of keys
              that may be used for this purpose and the intended outcome.
            </p>
            <table class='data'>
              <tr><th>Key</th><th>Outcome</th></tr>
              <tr>
                <td>chart</td>
                <td>
                  If a valid Google Charts url (starting with
                  http://chart.apis.google.com/chart?) is associated with this key then the chart
                  will be displayed on the scraper overview page.
                </td>
              </tr>
              <tr>
                <td>data_columns</td>
                <td>
                  If a list of column names is stored in this metadata item it will be used to decide
                  which columns will be displayed in the data table and in which order.
                </td>
              </tr>
              <tr>
                <td>map_columns</td>
                <td>
                  If a list of column names is stored in this metadata item it will be used to decide
                  which columns will be displayed in the map popups and in which order.

                  Note: 'latlng' must be included in this list for the points to be displayed on the map
                </td>
              </tr>
              <tr>
                <td>private_columns</td>
                <td>
                  If a list of column names is stored in this metadata item it will be used to hide
                  columns, in the data table and the map, from users who are not the owner of the
                  scraper.

                  Note: Making 'latlng' private will cause the points not to be displayed on the map
                </td>
              </tr>
              <tr>
                <td>num_data_points</td>
                <td>
                  Determines the maximum number of items that will be shown in the data table.
                  Default: {{settings.MAX_DATA_POINTS}}
                </td>
              </tr>
              <tr>
                <td>num_map_points</td>
                <td>
                  Determines the maximum number of points that will be shown on the map.
                  Default: {{settings.MAX_MAP_POINTS}}
                </td>
              </tr>
            </table>
        </div>

        <div class="section">
            <h2>Debugging</h2>
            <h3>print</h3>
            <p>To log what is happening, or as a general purpose tool to help
            debugging, use your language's standard print command. The output will appear
            in the "console" pane in the scraper code editor.
            </p>
        </div>
     </div>
{% endblock %}
